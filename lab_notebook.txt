2015-11-19
----------

* After long last, I finally created a separate WoCMan repo and I am now creating this notebook.  I maintained a paper log for a long time, but I don't remember at the moment where I put it.  This notebook can also be shared with Dan, Sid, and Emery so that they can follow along with my progress.

* So to summarize: WoCMan is a set of extensions to do parameter estimation in the AutoMan framework.  E.g., having the crowd answer questions like "What is the weight of this ox?"  In contrast with AutoMan, these kinds of tasks tend to be based on innumerable probability spaces rather than enumerable probability spaces.  In other words, people may largely agree on an estimate (2.01 vs 2.009) even through their responses are not exactly the same.

* Now let's talk about how WoCMan works.  The user interface for WoCMan is much like AutoMan.  You need to supply a little more information:
  1. An AutoMan function.
  2. A statistic (which is a function of the AutoMan return value)
  3. A confidence value.
  4. A confidence interval WIDTH.

* Let me motivate the point above with the following example.  You want to know how many M&Ms are in a jar (from a picture of a jar), plus or minus 10 M&Ms.  To use WoCMan, you might have the following:
  1. The AutoMan function:
  
  val HowManyMMs = (img: Image) => freetexts[Double] {
    image = img,
    text = "How many M&Ms are in this jar?"
  }
  
  2. The statistic:
  
  def mean(X: Array[Double]) = // whatever
  
  3. The confidence value: 0.95
  
  4. The confidence width: 20

* So the WoCMan function is defined like:

  def HowMany(img: Image) = wocman {
    automan = HowManyMMs(img),
    confidence = 0.95,
    ci = 20,
    statistic = Mean
  }
  
  Since a confidence level of 0.95 and the mean are pretty standard, we can make them defaults and write:
  
  def HowMany(img: Image) = wocman {
    automan = HowManyMMs(img),
    ci = 20
  }

* Now, what happens when you call this?  Ala,

  val mms = HowManyMMs(my_picture)
  
  [NOTE: skip down to 2015-11-25; the text below is incorrect]
  
  First, Scala runs "wocman", which is secretly a constructor, on your arguments, and it returns a WoCMan object.  What are your arguments?  Well: automan = HowManyMMs(img) and ci = 20.  Of course, it needs to evaluate HowManyMMs(img), which is a lambda, which calls "freetext", which is also a constructor.  This is how WoCMan/AutoMan controls when evaluation happens.  You're basically just calling constructors all the time.  In the background, worker threads start up to do the work.
  
  Note that the user never gave us a sample size.  WoCMan determines sample sizes automatically. To do this, it uses nonparametric bootstrap confidence interval estimation techniques (like the BCa bootstrap, described in "Bootstrap Confidence Intervals" by DiCiccio and Efron, 1996).  The beauty of these methods, as stated by DiCiccio & Efron is that "the algorithms are completely automatic, requiring no more thought for the maximum eigenvalue than the correlation coefficient or for any other parameter."  This allows us to construct reliable confidence intervals for arbitrary, user-defined statistics.  This approach allows us to specify exact confidence interval procedures for better accuracy when we recognize them (e.g., for the mean) and to have a theoretically sound fallback when we do not.  Of course, the downside of bootstrap procedures is that we always need empirical support to justify our estimates, so when using bootstrap procedures, WoCMan must always pay for some minimum, user-specified sample size.  The default sample size is the magic number 30.
  
  So a thread starts up and schedules the "how many M&Ms" job on MTurk with the default sample size.  When it comes back with some responses, the important part happens: WoCMan runs the "statistic" function on the answer and bootstrap confidence intervals, and performs the following test: is the bootstrap 1-alpha confidence interval at least as tight as the bound specified by the user?  If so, we're done, and WoCMan returns the parameter estimate and confidence interval.  If not, WoCMan obtains another sample.  The size of the new sample depends on user-specified policy.  A naive policy might be simply to double the total sample size.  Something more subtle might be based on rejecting a null hypothesis based on the current best estimate of the confidence interval along with the appropriate multiple comparisons correction.
  
  Finally, WoCMan returns the outcome, which is either an estimate of the statistic (in this case the mean) along with bootstrap confidence bounds, or else a program error along with the best estimate obtained so far (e.g., in the scenario where WoCMan runs out of money).
  
* So the important question moving forward is: what are our research questions?  The questions that motivate me are:
  1. Can a practical estimation language be built from 1) noisy crowdsourced responses and 2) without requiring deep statistical knowledge on the part of the programmer?
  2. How does it improve on the state of the art?
  3. Is the system accurate?
  4. Is the system efficient?
  5. How does it handle random-answering adversaries?
  6. Is it expressive and easy to use?
  
  I think that the following answers will satisfactorily answer the above questions:
  
  1. A working system will be existential proof that the answer is yes.
  2. We will compare against the state of the art--to our knowledge, WoCMan is the first automatic system for estimation tasks.
  3 & 4. We can answer this in a couple waus: 1) using simulation studies using parametric families with known exact estimation procedures.  Since Efron has already talked about this quite a bit, I don't think this needs to be very deep, and 2) using case studies drawn from our existing studies (e.g., school lunch surveys).
  5. As we noted before, and has been noted again recently (Gray and Suri, IIRC), random answers on MTurk are still a problem.  To what extent is WoCMan resilient to these threats to validity?  My thinking here is that we can try injecting low-quality answers into our results and see how it affects the output.  I don't have any great insights here yet, except that if answers are truly uniformly random, the only effect will be that WoCMan takes longer to deliver an answer, at least for estimates of the mean.  Other statistics may suffer.
  6. Having a small set of disparate apps that are unified by a common thread of parameter estimation would clearly demonstrate the effectiveness.  With regard to ease of use, there are two ways to attack this.  1) showing that the programs are declarative and short (ala the AutoMan paper) helps, but ideally we will 2) do a user study.  Obviously, #2 is more effort, but it is _very_ convincing.

2015-11-25
----------

* Let me take back some of the stuff above.  In retrospect, it doesn't make a whole lot of sense.  Here's a better formulation:

* WoCMan does parameter estimation, where a 'parameter' is an arbitrary function of the (true, unknown) distribution X.  In other words, parameter = F(X).  The function f is known in advance, but X is not.  But we can _estimate_ F(X) by drawing an i.i.d. sample (x_1,...,x_n) of size n from X---call it X_0---and computing an estimate F(X) from F(X_0).  Of course, this only gives us a point estimate of F(X).  Without confidence intervals, we can't say how likely it is that F(X_0) is far from F(X).

* This is where the bootstrap comes in.  Since sampling (e.g., gathering new responses from the crowd) is expensive, bootstrapping lets us approximate confidence intervals given the sample we already have.  The idea is to resample with replacement from X_0, and for each of the n _bootstrap replications_ of X_0---call them X_1,...,X_n---we compute F(X_i).  Now we have enough information, using quantiles, to estimate the variance of F(X) given our sample.  We can use this information to compute upper and lower bounds for any given confidence interval.  Nice walkthrough here: https://web.archive.org/web/20151125191138/http://www.unc.edu/courses/2007spring/biol/145/001/docs/lectures/Sep17.html

* But that still leaves one question unanswered: how did we determine our sample size in the first place?  For parametric forms, there are often formulas for choosing a sample size given a desired (tight) bound.  Unfortunately for us, we can't assume any parametric form: the function is user-specified.  So how do we find our n?  It turns out that the bootstrap can also be used to estimate this quantity.  The basic insight is statistical power.  Statistical power is how often a test rejects the null hypothesis given that the alternative hypothesis is true.  When it fails to reject, you get a false negative result.  Essentially, by controlling statistical power, you are controlling the accuracy of your test.  Power is a function of the distribution (which we can't change in our case), the statistic (which we also can't change), and the sample size (which we CAN change!).  So a power analysis will let us choose an n for a given false positive rate.  Now what?

* First, we need to define what we mean by "how often a test rejects the null hypothesis given that the alternative hypothesis is true."  This really makes us think: what is it that we want?  My opinion of what we want is the scenario I mentioned way back: "Give me the number of M&Ms in a jar +/- 5 M&Ms."  So we want to be able to say: "here's an estimate +/- 5 M&Ms with high confidence."  By "high confidence", we mean that were we to repeat the actual (high-cost) sampling procedure, the statistic we compute for each sample is _likely_ to fall with the region defined by estimate +/- 5.  So, more precisely, our null hypothesis is that |F(X_i) - F(X_j)| < epsilon, and our alternative hypothesis is |F(X_i) - F(X_j)| >= epsilon.  Second, we need a test.  Now, it turns out that there's a test designed for just this job: Mann-Whitney U.  Of course, to compute statistical power, we need to know how often we get it wrong when we KNOW that it is right.  One thought on how to do this is the following: 1. do a pilot study, 2. compute F, 3. "fudge F" by shifting it +/- 5, 4. bootstrap B replications of F, 5. for each F, compute Mann-Whitney, 6. compute the proportion of correctly-rejected replications, 7. finally: if the proportion is greater than our desired power, then the size n of our pilot is sufficient, if not, we increase n.

* Of course, every time we recalculate the sample size, we will ALSO need to adjust our confidence level for multiple comparisons.  This should be the same procedure that we currently use in AutoMan.

* One last thought: confidence level and error are more or less interchangeable here.  So even though AutoMan currently accepts a confidence parameter, a user could instead provide the error that indicates (say) the 95% confidence interval.  I think that makes more sense intuitively, and it would be a nice usability concession.