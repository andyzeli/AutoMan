2015-11-19
----------

* After long last, I finally created a separate WoCMan repo and I am now creating this notebook.  I maintained a paper log for a long time, but I don't remember at the moment where I put it.  This notebook can also be shared with Dan, Sid, and Emery so that they can follow along with my progress.

* So to summarize: WoCMan is a set of extensions to do parameter estimation in the AutoMan framework.  E.g., having the crowd answer questions like "What is the weight of this ox?"  In contrast with AutoMan, these kinds of tasks tend to be based on innumerable probability spaces rather than enumerable probability spaces.  In other words, people may largely agree on an estimate (2.01 vs 2.009) even through their responses are not exactly the same.

* Now let's talk about how WoCMan works.  The user interface for WoCMan is much like AutoMan.  You need to supply a little more information:
  1. An AutoMan function.
  2. A statistic (which is a function of the AutoMan return value)
  3. A confidence value.
  4. A confidence interval WIDTH.

* Let me motivate the point above with the following example.  You want to know how many M&Ms are in a jar (from a picture of a jar), plus or minus 10 M&Ms.  To use WoCMan, you might have the following:
  1. The AutoMan function:
  
  val HowManyMMs = (img: Image) => freetexts[Double] {
    image = img,
    text = "How many M&Ms are in this jar?"
  }
  
  2. The statistic:
  
  def mean(X: Array[Double]) = // whatever
  
  3. The confidence value: 0.95
  
  4. The confidence width: 20

* So the WoCMan function is defined like:

  def HowMany(img: Image) = wocman {
    automan = HowManyMMs(img),
    confidence = 0.95,
    ci = 20,
    statistic = Mean
  }
  
  Since a confidence level of 0.95 and the mean are pretty standard, we can make them defaults and write:
  
  def HowMany(img: Image) = wocman {
    automan = HowManyMMs(img),
    ci = 20
  }

* Now, what happens when you call this?  Ala,

  val mms = HowManyMMs(my_picture)
  
  [NOTE: skip down to 2015-11-25; the text below is incorrect]
  
  First, Scala runs "wocman", which is secretly a constructor, on your arguments, and it returns a WoCMan object.  What are your arguments?  Well: automan = HowManyMMs(img) and ci = 20.  Of course, it needs to evaluate HowManyMMs(img), which is a lambda, which calls "freetext", which is also a constructor.  This is how WoCMan/AutoMan controls when evaluation happens.  You're basically just calling constructors all the time.  In the background, worker threads start up to do the work.
  
  Note that the user never gave us a sample size.  WoCMan determines sample sizes automatically. To do this, it uses nonparametric bootstrap confidence interval estimation techniques (like the BCa bootstrap, described in "Bootstrap Confidence Intervals" by DiCiccio and Efron, 1996).  The beauty of these methods, as stated by DiCiccio & Efron is that "the algorithms are completely automatic, requiring no more thought for the maximum eigenvalue than the correlation coefficient or for any other parameter."  This allows us to construct reliable confidence intervals for arbitrary, user-defined statistics.  This approach allows us to specify exact confidence interval procedures for better accuracy when we recognize them (e.g., for the mean) and to have a theoretically sound fallback when we do not.  Of course, the downside of bootstrap procedures is that we always need empirical support to justify our estimates, so when using bootstrap procedures, WoCMan must always pay for some minimum, user-specified sample size.  The default sample size is the magic number 30.
  
  So a thread starts up and schedules the "how many M&Ms" job on MTurk with the default sample size.  When it comes back with some responses, the important part happens: WoCMan runs the "statistic" function on the answer and bootstrap confidence intervals, and performs the following test: is the bootstrap 1-alpha confidence interval at least as tight as the bound specified by the user?  If so, we're done, and WoCMan returns the parameter estimate and confidence interval.  If not, WoCMan obtains another sample.  The size of the new sample depends on user-specified policy.  A naive policy might be simply to double the total sample size.  Something more subtle might be based on rejecting a null hypothesis based on the current best estimate of the confidence interval along with the appropriate multiple comparisons correction.
  
  Finally, WoCMan returns the outcome, which is either an estimate of the statistic (in this case the mean) along with bootstrap confidence bounds, or else a program error along with the best estimate obtained so far (e.g., in the scenario where WoCMan runs out of money).
  
* So the important question moving forward is: what are our research questions?  The questions that motivate me are:
  1. Can a practical estimation language be built from 1) noisy crowdsourced responses and 2) without requiring deep statistical knowledge on the part of the programmer?
  2. How does it improve on the state of the art?
  3. Is the system accurate?
  4. Is the system efficient?
  5. How does it handle random-answering adversaries?
  6. Is it expressive and easy to use?
  
  I think that the following answers will satisfactorily answer the above questions:
  
  1. A working system will be existential proof that the answer is yes.
  2. We will compare against the state of the art--to our knowledge, WoCMan is the first automatic system for estimation tasks.
  3 & 4. We can answer this in a couple waus: 1) using simulation studies using parametric families with known exact estimation procedures.  Since Efron has already talked about this quite a bit, I don't think this needs to be very deep, and 2) using case studies drawn from our existing studies (e.g., school lunch surveys).
  5. As we noted before, and has been noted again recently (Gray and Suri, IIRC), random answers on MTurk are still a problem.  To what extent is WoCMan resilient to these threats to validity?  My thinking here is that we can try injecting low-quality answers into our results and see how it affects the output.  I don't have any great insights here yet, except that if answers are truly uniformly random, the only effect will be that WoCMan takes longer to deliver an answer, at least for estimates of the mean.  Other statistics may suffer.
  6. Having a small set of disparate apps that are unified by a common thread of parameter estimation would clearly demonstrate the effectiveness.  With regard to ease of use, there are two ways to attack this.  1) showing that the programs are declarative and short (ala the AutoMan paper) helps, but ideally we will 2) do a user study.  Obviously, #2 is more effort, but it is _very_ convincing.

2015-11-25
----------

* Let me take back some of the stuff above.  In retrospect, it doesn't make a whole lot of sense.  Here's a better formulation:

* WoCMan does parameter estimation, where a 'parameter' is an arbitrary function of the (true, unknown) distribution X.  In other words, parameter = F(X).  The function f is known in advance, but X is not.  But we can _estimate_ F(X) by drawing an i.i.d. sample (x_1,...,x_n) of size n from X---call it X_0---and computing an estimate F(X) from F(X_0).  Of course, this only gives us a point estimate of F(X).  Without confidence intervals, we can't say how likely it is that F(X_0) is far from F(X).

* This is where the bootstrap comes in.  Since sampling (e.g., gathering new responses from the crowd) is expensive, bootstrapping lets us approximate confidence intervals given the sample we already have.  The idea is to resample with replacement from X_0, and for each of the n _bootstrap replications_ of X_0---call them X_1,...,X_n---we compute F(X_i).  Now we have enough information, using quantiles, to estimate the variance of F(X) given our sample.  We can use this information to compute upper and lower bounds for any given confidence interval.  Nice walkthrough here: https://web.archive.org/web/20151125191138/http://www.unc.edu/courses/2007spring/biol/145/001/docs/lectures/Sep17.html

* But that still leaves one question unanswered: how did we determine our sample size in the first place?  For parametric forms, there are often formulas for choosing a sample size given a desired (tight) bound.  Unfortunately for us, we can't assume any parametric form: the function is user-specified.  So how do we find our n?  It turns out that the bootstrap can also be used to estimate this quantity.  The basic insight is statistical power.  Statistical power is how often a test rejects the null hypothesis given that the alternative hypothesis is true.  When it fails to reject, you get a false negative result.  Essentially, by controlling statistical power, you are controlling the accuracy of your test.  Power is a function of the distribution (which we can't change in our case), the statistic (which we also can't change), and the sample size (which we CAN change!).  So a power analysis will let us choose an n for a given false positive rate.  Now what?

* First, we need to define what we mean by "how often a test rejects the null hypothesis given that the alternative hypothesis is true."  This really makes us think: what is it that we want?  My opinion of what we want is the scenario I mentioned way back: "Give me the number of M&Ms in a jar +/- 5 M&Ms."  So we want to be able to say: "here's an estimate +/- 5 M&Ms with high confidence."  By "high confidence", we mean that were we to repeat the actual (high-cost) sampling procedure, the statistic we compute for each sample is _likely_ to fall with the region defined by estimate +/- 5.  So, more precisely, our null hypothesis is that |F(X_i) - F(X_j)| < epsilon, and our alternative hypothesis is |F(X_i) - F(X_j)| >= epsilon.  Second, we need a test.  Now, it turns out that there's a test designed for just this job: Mann-Whitney U.  Of course, to compute statistical power, we need to know how often we get it wrong when we KNOW that it is right.  One thought on how to do this is the following: 1. do a pilot study, 2. compute F, 3. "fudge F" by shifting it +/- 5, 4. bootstrap B replications of F, 5. for each F, compute Mann-Whitney, 6. compute the proportion of correctly-rejected replications, 7. finally: if the proportion is greater than our desired power, then the size n of our pilot is sufficient, if not, we increase n.

* Of course, every time we recalculate the sample size, we will ALSO need to adjust our confidence level for multiple comparisons.  This should be the same procedure that we currently use in AutoMan.

* One last thought: confidence level and error are more or less interchangeable here.  So even though AutoMan currently accepts a confidence parameter, a user could instead provide the error that indicates (say) the 95% confidence interval.  I think that makes more sense intuitively, and it would be a nice usability concession.

2015-11-27
----------

* OK, I thought about this all day yesterday.  Something was bothering me about the above formulation, even though it is definitely better than the one before it.  Basically: the power analysis uses a test whereas the estimation does not.  On one level, this makes sense: estimation is not about testing.  I think the reason why I conflated the two is that estimation procedures produce statistics which can themselves be tested.  Anyway, this is a bit of an issue because the bootstrap power analysis only tells you one thing: the power of the TEST.  So let's step back a bit and talk about what we want.

* What we want is for a user to be able to estimate some parameter of interest.  They do this by i.i.d. sampling and computing a statistic.  Whether the statistic in question is a good estimator for the parameter is an interesting question but ultimately, not my problem.

* Of course, the issue with the above is that it's just a point estimate.  Usually, a person wants to know whether an estimate is "good".  One way of understanding this goodness is in terms of the error: what range of values are also likely?  So a user could ask for a 95% confidence interval around their estimate, and then when we give them the estimate and the interval, they know the range of values that the true parameter is likely to take, although we still have not given them any indication of the probability of those values (i.e., like a PDF).  A query of this form might look like "estimate X with a 95% CI."

* Another, possibly more intuitive, form of the query above is for a user to ask for an estimate and the confidence level of a particular error bound.  This is because for a sampling distribution, the confidence level and the confidence interval are duals.  A query of this form might look like "estimate X +/- 5."  In this latter case, of course, we will need to tell them the confidence level otherwise their bound is meaningless.

* The bootstrap (in its many forms) can actually produce estimates and their confidence intervals.  Whether the bootstrapped estimate is close to the true parameter depends on a few factors: 1. the estimator (function), 2. the number of bootstrap replications, and 3. the sample size.  For WoCMan, #1 is out of our control.  And I am largely content with leaving #2 as a user parameter, setting it fairly high by default, or just by making it the function of a time bound (i.e., an anytime algorithm).  But #3... how do we determine this?  This is where my idea about doing a power analysis came in.  With a power analysis we can find a sample size that puts the procedure over some user-defined quality threshold (i.e., the false negative rate).  But the problem is... power analyses are for TESTS.

* There are two possible approaches for this:
  1. Give up on finding a sample size.  The USER specifies the sample size.  Or better still, they specify a fixed budget, and we do our best with those resources.
  2. We try to elicit some kind of test from the user so that we can do a power analysis.  One interesting approach is something called a "testimator".  A testimator is defined an estimator whose value depends on the result of a test for statistical significance.  So, e.g., a user doesn't just say "estimate the number of M&Ms in the jar with a 95% CI" or "estimate the number of M&Ms in the jar +/- 5 M&Ms," they actually give us something that we can test and confirm or deny the truth of.  So, e.g., "the estimated number of M&Ms in the jar +/- 5 M&Ms is likely to be true with a 95% probability."  Now here's something that we can actually work with:
    a. Take an initial (small, "pilot") sample.
    b. Construct a true alternative hypothesis using the estimate (e.g., by shifting the null hypothesis to make a claim about the estimate with respect to the user-specified confidence interval).
    c. Resample with replacement from the pilot sample B times.
    d. For each b of B resamples, compute the outcome of the test (is the estimate within the bounds?).
    e. Count the number of times the null hypothesis is rejected.
    f. Increase the sample size, correct the significance level for multiple comparisons, and goto a. until either i. the test passes the threshold, or ii. the user runs out of money.
    
* Gotta go.  Guests here for dinner.  But I believe the above satisfactorily deals with the test/estimation conflation and also should satisfy Emery that we are not "breaking new statistical ground."